{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw\n",
        "➡ CO (42101)\n",
        "\n",
        "This is the link to the original dataset the model needs to be trained on\n",
        "\n",
        "**The data file I used in this notebook is a reduced version of the original to run faster on colab\n",
        "\n",
        "**Original(1000000+ lines) ThisVersion(200 lines)"
      ],
      "metadata": {
        "id": "1sgJf7hYXWV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3VVNTsdYmO98"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read csv\n",
        "\n",
        "df = pd.read_csv(\"hourly_42101_2021_new.csv\")"
      ],
      "metadata": {
        "id": "zKJM1-YGm0XB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check df\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "FY8g3AjOPTe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping all irrelevant columns (only kept the necessary ones)\n",
        "\n",
        "df = df.drop([\"State Code\", \"County Code\", \"Parameter Code\", \"POC\", \"Datum\",\n",
        "              \"Site Num\", \"MDL\", \"Uncertainty\", \"Qualifier\", \"Method Type\",\n",
        "              \"Method Code\", \"Method Name\", \"Date of Last Change\",\n",
        "              \"Parameter Name\", \"Units of Measure\", \"State Name\", \"County Name\",\n",
        "              \"Date GMT\", \"Time GMT\"], axis = 'columns')"
      ],
      "metadata": {
        "id": "SQXRq20AcGbc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking drop\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "aUClFTim0cgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First function converts date into day of year, and then normalizes it to scale of 0 to 1\n",
        "#Second function converts time of day, into hour of day, and then normalizes it to scale of 0 to 1\n",
        "\n",
        "def adjust_date(arr):\n",
        "  adjusted_date_local = []\n",
        "\n",
        "  for date in arr:\n",
        "    temp = date.split(\"/\")\n",
        "    curr = temp[2] + \"-\" + temp[0] + \"-\"  + temp[1]\n",
        "    period = pd.Period(curr)\n",
        "    adjusted_date_local.append(int(period.day_of_year)/365)\n",
        "\n",
        "  return adjusted_date_local\n",
        "\n",
        "def adjust_time(arr):\n",
        "  adjusted_time_local = []\n",
        "\n",
        "  for time in arr:\n",
        "    strTime = time.replace(\":\", \".\")\n",
        "    adjusted_time_local.append(float(strTime)/24)\n",
        "\n",
        "  return adjusted_time_local"
      ],
      "metadata": {
        "id": "sqdFD_QcYcD-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the new adjusted columns by applying the functions to existing values\n",
        "#Deleting old non-formatted columns\n",
        "\n",
        "df[\"Date Local (adjusted)\"] = adjust_date(df[\"Date Local\"])\n",
        "df[\"Time Local (adjusted)\"] = adjust_time(df[\"Time Local\"])\n",
        "\n",
        "df = df.drop([\"Date Local\", \"Time Local\"], axis = 'columns')"
      ],
      "metadata": {
        "id": "1XpOaKTBMod-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that normalizes the longitude and latitude to a scale of 0 to 1\n",
        "\n",
        "def adjust_long_lat(arr):\n",
        "  adjusted_long_lat = []\n",
        "\n",
        "  for pos in arr:\n",
        "    adjusted_long_lat.append(float(pos)/180)\n",
        "\n",
        "  return adjusted_long_lat"
      ],
      "metadata": {
        "id": "5Dm-yhquck2P"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Once again adding the new adjusted columns and deleting old non-formatted columns\n",
        "\n",
        "df[\"Latitude (adjusted)\"] = adjust_long_lat(df[\"Latitude\"])\n",
        "df[\"Longitude (adjusted)\"] = adjust_long_lat(df[\"Longitude\"])\n",
        "\n",
        "df = df.drop([\"Latitude\", \"Longitude\"], axis = 'columns')"
      ],
      "metadata": {
        "id": "JDBK06HUdEl3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking new edits to df\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ske8trdZdf_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turning \"Sample Measurement\" column into target (output) array and dropping from df\n",
        "\n",
        "target = df[\"Sample Measurement\"]\n",
        "df = df.drop(\"Sample Measurement\", axis = 'columns')"
      ],
      "metadata": {
        "id": "yc04ldt5dxyo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting dataset into 80/20 train/test split and checking shape\n",
        "\n",
        "split_point = int(0.8 * len(df))\n",
        "x_train = df[0:split_point]\n",
        "x_test = df[split_point:]\n",
        "y_train = target[0:split_point]\n",
        "y_test = target[split_point:]\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "e52Ngmuz50Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting df DataFrame objects to Numpy, so reshape function can be applied\n",
        "\n",
        "x_train = x_train.to_numpy()\n",
        "x_test = x_test.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "S7IQ5ovG8FXI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping current data format to fit 3D input array for LSTM\n",
        "#(num_of_samples, num_timesteps, num_features)\n",
        "#(total # of data points, 1 incremental timestep, 4 input categories)\n",
        "\n",
        "x_train_reshaped = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "x_test_reshaped = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
      ],
      "metadata": {
        "id": "MeuJXUEpIcg4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building LSTM model\n",
        "\n",
        "model = Sequential() #For linear stack of layers\n",
        "model.add(LSTM(x_train.shape[0], input_shape=(1, x_train.shape[1]))) #LSTM dimensions (≈200, 1, 4)\n",
        "model.add(Dense(1)) #Fully connected output layer with one node (for \"Sample Measurements\")"
      ],
      "metadata": {
        "id": "sEMQZ1saIhQg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling and fitting dataset to model\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam') #MSE loss function since this is a regression problem\n",
        "\n",
        "model.fit(x_train_reshaped, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "6dhN4nShL3kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing accuracy\n",
        "\n",
        "loss = model.evaluate(x_test_reshaped, y_test)\n",
        "print('Test loss:', 100 * loss, \"%\")"
      ],
      "metadata": {
        "id": "FqhvH5y_MADz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definitely needs fine-tuning and editing.\n",
        "\n",
        "Extremely sparse data being used also contributed to terrible accuracy"
      ],
      "metadata": {
        "id": "w4718_pQhMqL"
      }
    }
  ]
}